{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Imports\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation\n",
    "import torch\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132194db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo pr√©-treinado de segmenta√ß√£o sem√¢ntica chamado deeplabv3_mobilenet_v2_1.0_513\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/deeplabv3_mobilenet_v2_1.0_513\")\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"google/deeplabv3_mobilenet_v2_1.0_513\")\n",
    "\n",
    "# Essa fun√ß√£o pega um frame de v√≠deo (em formato NumPy RGB) \n",
    "# retorna um mapa de segmenta√ß√£o, onde cada pixel tem um valor correspondente √† classe detectada (ex: rua, pessoa, carro...)\n",
    "\n",
    "def segment_frame_hf(np_img):\n",
    "    inputs = processor(images=np_img, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    seg = processor.post_process_semantic_segmentation(outputs, target_sizes=[np_img.shape[:2]])[0]\n",
    "    return np.array(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1909936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar segmenta√ß√£o sem√¢ntica em 1 frame a cada 5 segundos, salvando o tempo de processamento de cada frame.\n",
    "\n",
    "\n",
    "video_path = '/content/drive/MyDrive/Atividade 4 - VisaÃÉo Computacional/cidade'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"\\nErro ao abrir o v√≠deo.\\n\")\n",
    "else:\n",
    "    print(\"\\nV√≠deo carregado com sucesso.\\n\")\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"Total de frames: {total_frames}\")  # Total de frames no v√≠deo\n",
    "    print(f\"FPS (frames por segundo): {fps}\")  # Quantos frames existem por segundo\n",
    "\n",
    "# Processar a cada 5 segundos\n",
    "step = int(fps * 5)\n",
    "results = []\n",
    "\n",
    "print(f\"\\nüü¶ Processando 1 frame a cada 5 segundos (step = {step} frames)\")\n",
    "steps_segundos = [1, 2, 5]\n",
    "resultados_dict = {}\n",
    "\n",
    "for step_segundos in steps_segundos:\n",
    "    print(f\"\\nüü¶ Processando 1 frame a cada {step_segundos} segundos\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    step = int(fps * step_segundos)\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for i in tqdm(range(0, total_frames, step), desc=f\"{step_segundos}s\"):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        print(f\"Processando frame {i} - Leitura OK? {ret}\")\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        start = time.time()\n",
    "        seg = segment_frame_hf(rgb_frame)\n",
    "        end = time.time()\n",
    "\n",
    "        tempo = end - start\n",
    "        resultados.append(tempo)\n",
    "        print(f\"Frame {i} processado em {tempo:.2f}s\")\n",
    "\n",
    "    resultados_dict[step_segundos] = resultados\n",
    "    cap.release()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d888e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Total de frames:\", total_frames)\n",
    "print(\"FPS:\", fps)\n",
    "print(\"Step:\", step)\n",
    "print(\"Chaves dispon√≠veis em resultados_dict:\", resultados_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae16b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico comparando o tempo m√©dio de processamento em diferentes taxas\n",
    "media_por_step = [np.mean(resultados_dict[s]) for s in steps_segundos]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(steps_segundos, media_por_step, marker='o', linestyle='-')\n",
    "for x, y in zip(steps_segundos, media_por_step):\n",
    "    plt.text(x, y + 0.02, f\"{y:.2f}s\", ha='center')\n",
    "plt.xlabel(\"Intervalo de captura (segundos)\")\n",
    "plt.ylabel(\"Tempo m√©dio de processamento (s)\")\n",
    "plt.title(\"Impacto da taxa de amostragem no tempo de processamento\")\n",
    "plt.grid(True)\n",
    "plt.xticks(steps_segundos)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_indices = list(range(0, total_frames, step))[:3] # Lista com os √≠ndices dos frames a serem exibidos (primeiros 3 frames processados)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for idx, i in enumerate(frame_indices):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    ret, frame = cap.read()\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    seg = segment_frame_hf(rgb)\n",
    "    seg_color = cv2.applyColorMap((seg * 20).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    seg_color = cv2.resize(seg_color, (frame.shape[1], frame.shape[0]))\n",
    "    overlay = cv2.addWeighted(rgb, 0.6, seg_color, 0.4, 0)\n",
    "\n",
    "    plt.subplot(1, 3, idx + 1)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(f'Frame {i}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(\"Exemplos de segmenta√ß√£o\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/content/drive/MyDrive/trab/Theodor_segmentado.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "\n",
    "fps = 5 # velocidade de reprodu√ß√£o\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Ler o primeiro frame para pegar altura e largura\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    raise ValueError(\"\\nN√£o foi poss√≠vel ler o primeiro frame do v√≠deo.\")\n",
    "\n",
    "height, width, _ = frame.shape\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Processar o v√≠deo completo\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "for i in range(0, total_frames, step):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    seg = segment_frame_hf(rgb)\n",
    "    \n",
    "    # Converter a segmenta√ß√£o para cores\n",
    "    seg_color = cv2.applyColorMap((seg * 20).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    seg_color = cv2.resize(seg_color, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Sobrepor a segmenta√ß√£o na imagem original\n",
    "    overlay = cv2.addWeighted(frame, 0.6, seg_color, 0.4, 0)\n",
    "    out.write(overlay)\n",
    "\n",
    "out.release()\n",
    "print(f\"\\n Novo v√≠deo salvo em: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
